### CODE GENERATED BY Claude Code - modified by Qasim Wani

from __future__ import annotations

import argparse
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
from typing import Dict, Iterable, List

from ..api import detect
from ..utils.image_utils import (calculate_focal_diou,
                                 calculate_iou_with_offset,
                                 calculate_modified_diou)

SUPPORTED_VANILLA_AGENTS = {
    "xai_vanilla_reasoning",
    "openai_vanilla_reasoning",
}

# These tasks will be run serially to avoid resource contention
SERIAL_TASKS = set()


def _clean_result(result: Dict) -> Dict:
    """Remove visualization images from result to reduce storage."""
    return {
        key: value
        for key, value in result.items()
        if key not in {"visualized_image", "original_image"}
    }


def _compute_metrics(payload: Dict, sample: Dict) -> Dict[str, float]:
    """Compute evaluation metrics for predictions."""
    metrics = {"iou": 0.0, "modified_diou": 0.0, "max_focal_diou": 0.0}

    if not isinstance(payload, dict):
        return metrics

    annotations = sample.get("annotations")
    if not annotations:
        return metrics

    ground_truths = [
        tuple(map(float, ann[:4]))
        for ann in annotations
        if isinstance(ann, (list, tuple)) and len(ann) >= 4
    ]
    if not ground_truths:
        return metrics

    crop_metadata = payload.get("crop_metadata") or {}
    zoom_source = (
        crop_metadata.get("global_crops")
        or payload.get("global_crops")
        or []
    )
    zoom_boxes = [
        tuple(map(float, crop[:4]))
        for crop in zoom_source
        if isinstance(crop, (list, tuple)) and len(crop) >= 4
    ]

    confidences = (
        crop_metadata.get("confidences")
        or payload.get("confidences")
        or []
    )
    if confidences:
        # Skip first confidence (original image)
        metrics["average_confidence"] = (
            float(sum(confidences[1:]) / len(confidences[1:]))
            if len(confidences) > 1
            else 0.0
        )

    if zoom_boxes:
        metrics["max_focal_diou"] = float(
            max(
                calculate_focal_diou(gt_box, zoom_boxes)
                for gt_box in ground_truths
            )
        )

    predictions = payload.get("bboxs")
    if not predictions:
        return metrics

    resolution = sample.get("metadata", {}).get("resolution")
    if not (isinstance(resolution, (list, tuple)) and len(resolution) == 2):
        return metrics
    image_size = (int(resolution[0]), int(resolution[1]))

    best_iou = -1.0
    best_diou = None

    for pred in predictions:
        if not (isinstance(pred, (list, tuple)) and len(pred) == 4):
            continue
        pred_box = tuple(map(float, pred))
        for gt_box in ground_truths:
            iou = calculate_iou_with_offset(pred_box, gt_box, image_size)
            diou = calculate_modified_diou(pred_box, gt_box)
            if iou > best_iou:
                best_iou = iou
                best_diou = diou

    if best_iou >= 0:
        metrics["iou"] = float(best_iou)
        if best_diou is not None:
            metrics["modified_diou"] = float(best_diou)

    return metrics


def run_normalized_benchmark(
    agents: List[str],
    data_path: str,
    save_location: str,
    num_workers: int | None = None,
) -> None:
    """
    Run benchmark comparing normalized vs unnormalized coordinates.

    For each agent in the list, creates two variants:
    - {agent}: runs with default (unnormalized) coordinates
    - {agent}_normalized: runs with is_normalized_coordinates=True
    """
    # Validate agents
    invalid_agents = [a for a in agents if a not in SUPPORTED_VANILLA_AGENTS]
    if invalid_agents:
        raise ValueError(
            f"Unsupported agents: {invalid_agents}. "
            f"Only vanilla reasoning agents are supported: "
            f"{SUPPORTED_VANILLA_AGENTS}"
        )

    with open(data_path) as fh:
        samples = json.load(fh)

    output_root = Path(save_location)
    output_root.mkdir(parents=True, exist_ok=True)

    # Create agent variants
    agent_variants = []
    for agent in agents:
        # Add unnormalized variant
        agent_variants.append({
            "name": agent,
            "task_type": agent,
            "normalized": False,
        })
        # Add normalized variant
        agent_variants.append({
            "name": f"{agent}_normalized",
            "task_type": agent,
            "normalized": True,
        })

    outputs: Dict[str, List[Dict]] = {
        variant["name"]: [None] * len(samples)
        for variant in agent_variants
    }

    def _execute(
        variant: Dict, sample: Dict, idx: int
    ) -> tuple[str, int, Dict]:
        """Execute detection for a single agent variant and sample."""
        agent_name = variant["name"]
        task_type = variant["task_type"]
        is_normalized = variant["normalized"]

        prompt = sample.get("metadata", {}).get("prompt", "")
        task_kwargs = sample.get("metadata", {}).get("task_kwargs")

        # Create kwargs with normalized flag
        if isinstance(task_kwargs, dict):
            kwargs = dict(task_kwargs)
        else:
            kwargs = {} if task_kwargs is None else task_kwargs

        # Add the normalized coordinates flag
        kwargs["is_normalized_coordinates"] = is_normalized

        try:
            result = detect(
                image_path=sample["image_url"],
                object_of_interest=prompt,
                task_type=task_type,
                task_kwargs=kwargs,
                return_overlay_images=False,
            )
            payload = _clean_result(result)
            # Add flag to payload for tracking
            payload["is_normalized_coordinates"] = is_normalized
        except Exception as exc:  # noqa: BLE001
            payload = {
                "error": str(exc),
                "is_normalized_coordinates": is_normalized,
            }

        record = {
            "image_url": sample.get("image_url"),
            "metadata": sample.get("metadata"),
            "annotations": sample.get("annotations"),
            "model_outputs": payload,
            "metrics": _compute_metrics(payload, sample),
            "variant_info": {
                "agent": task_type,
                "is_normalized": is_normalized,
            },
        }
        return agent_name, idx, record

    # Determine which agents to run in parallel
    parallel_variants = [
        v for v in agent_variants
        if v["task_type"] not in SERIAL_TASKS
    ]
    serial_variants = [
        v for v in agent_variants
        if v not in parallel_variants
    ]

    worker_count = max(
        1,
        num_workers
        or min(32, len(samples) * max(1, len(parallel_variants))),
    )

    # Run parallel agents
    if parallel_variants:
        with ThreadPoolExecutor(max_workers=worker_count) as executor:
            futures = [
                executor.submit(_execute, variant, sample, idx)
                for variant in parallel_variants
                for idx, sample in enumerate(samples)
            ]
            for future in as_completed(futures):
                agent_name, idx, record = future.result()
                outputs[agent_name][idx] = record
                print(
                    f"Completed {agent_name} - "
                    f"Sample {idx + 1}/{len(samples)}"
                )

    # Run serial agents
    for variant in serial_variants:
        for idx, sample in enumerate(samples):
            agent_name = variant["name"]
            _, _, record = _execute(variant, sample, idx)
            outputs[agent_name][idx] = record
            print(
                f"Completed {agent_name} - "
                f"Sample {idx + 1}/{len(samples)}"
            )

    # Save outputs for each agent variant
    for agent_name, entries in outputs.items():
        agent_folder = output_root / agent_name
        agent_folder.mkdir(parents=True, exist_ok=True)
        output_file = agent_folder / "output.json"
        with open(output_file, "w") as fh:
            json.dump(entries, fh, indent=2)
        print(f"Saved results to {output_file}")

    # Create summary comparison
    print("\nCreating comparison summary...")
    summary = create_comparison_summary(outputs, agents)
    summary_file = output_root / "normalized_comparison_summary.json"
    with open(summary_file, "w") as fh:
        json.dump(summary, fh, indent=2)
    print(f"Saved comparison summary to {summary_file}")


def create_comparison_summary(
    outputs: Dict[str, List[Dict]], base_agents: List[str]
) -> Dict:
    """Create a summary comparing normalized vs unnormalized results."""
    summary = {"agents": {}, "overall_statistics": {}}

    for agent in base_agents:
        unnormalized_key = agent
        normalized_key = f"{agent}_normalized"

        if (
            unnormalized_key not in outputs
            or normalized_key not in outputs
        ):
            continue

        agent_summary = {
            "unnormalized": {"metrics": []},
            "normalized": {"metrics": []},
            "comparison": {},
        }

        # Collect metrics for both variants
        for record in outputs[unnormalized_key]:
            if record and "metrics" in record:
                agent_summary["unnormalized"]["metrics"].append(
                    record["metrics"]
                )

        for record in outputs[normalized_key]:
            if record and "metrics" in record:
                agent_summary["normalized"]["metrics"].append(
                    record["metrics"]
                )

        # Calculate average metrics
        for variant in ["unnormalized", "normalized"]:
            metrics_list = agent_summary[variant]["metrics"]
            if metrics_list:
                avg_metrics = {}
                metric_names = metrics_list[0].keys()
                for metric in metric_names:
                    values = [
                        m[metric] for m in metrics_list
                        if metric in m and m[metric] is not None
                    ]
                    if values:
                        avg_metrics[f"avg_{metric}"] = (
                            sum(values) / len(values)
                        )
                agent_summary[variant]["average_metrics"] = avg_metrics

        # Calculate improvement
        if (
            "average_metrics" in agent_summary["unnormalized"]
            and "average_metrics" in agent_summary["normalized"]
        ):
            unnorm_avg = agent_summary["unnormalized"]["average_metrics"]
            norm_avg = agent_summary["normalized"]["average_metrics"]

            comparison = {}
            for metric in unnorm_avg:
                if metric in norm_avg:
                    diff = norm_avg[metric] - unnorm_avg[metric]
                    if unnorm_avg[metric] != 0:
                        pct_change = (diff / unnorm_avg[metric]) * 100
                    else:
                        pct_change = 100 if diff > 0 else 0
                    comparison[metric] = {
                        "difference": diff,
                        "percent_change": pct_change,
                        "improved": diff > 0,
                    }
            agent_summary["comparison"] = comparison

        summary["agents"][agent] = agent_summary

    # Calculate overall statistics
    all_comparisons = []
    for agent_data in summary["agents"].values():
        if "comparison" in agent_data and agent_data["comparison"]:
            all_comparisons.append(agent_data["comparison"])

    if all_comparisons:
        overall = {}
        # Get all metric names from first comparison
        metric_names = all_comparisons[0].keys()
        for metric in metric_names:
            improvements = [
                comp[metric]["percent_change"]
                for comp in all_comparisons
                if metric in comp
            ]
            if improvements:
                overall[metric] = {
                    "avg_improvement": sum(improvements) / len(improvements),
                    "max_improvement": max(improvements),
                    "min_improvement": min(improvements),
                }
        summary["overall_statistics"] = overall

    return summary


def parse_args(argv: Iterable[str] | None = None):
    parser = argparse.ArgumentParser(
        description="Run normalized vs unnormalized coordinate benchmark"
    )
    parser.add_argument(
        "--agents",
        nargs="+",
        required=True,
        choices=list(SUPPORTED_VANILLA_AGENTS),
        help="Vanilla reasoning agent names to evaluate",
    )
    parser.add_argument(
        "--data",
        required=True,
        help="Path to dataset JSON file",
    )
    parser.add_argument(
        "--save-location",
        required=True,
        help="Directory to store benchmark outputs",
    )
    parser.add_argument(
        "--num-workers",
        type=int,
        default=None,
        help="Worker threads for parallel execution (auto if omitted)",
    )
    return parser.parse_args(argv)


def main(argv: Iterable[str] | None = None) -> None:
    args = parse_args(argv)
    print(f"Running normalized coordinate benchmark with agents: {args.agents}")
    print(f"Each agent will be tested with both normalized and unnormalized coordinates")
    run_normalized_benchmark(
        args.agents,
        args.data,
        args.save_location,
        args.num_workers,
    )


if __name__ == "__main__":
    main()